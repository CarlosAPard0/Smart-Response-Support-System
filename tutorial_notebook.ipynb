{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5509730",
   "metadata": {},
   "source": [
    "# Guía práctica: Chatbot IA con RAG\n",
    "\n",
    "Este notebook es una versión tutorial y limpia del examen original.\n",
    "Presenta tres stages bien definidos:\n",
    "\n",
    "1. Ingesta y embeddings\n",
    "2. Índice y recuperación (RAG)\n",
    "3. Chatbot y generación (fallback)\n",
    "\n",
    "Cada stage incluye explicación, código reutilizable y pruebas mínimas. El notebook está en español y preparado para ejecutarse en modo \"mock\" (sin clave API) o en modo real si se configura `OPENAI_API_KEY`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ef3e45",
   "metadata": {},
   "source": [
    "## 1) Instalación y entorno\n",
    "\n",
    "Instalación rápida (PowerShell) y comprobación de versiones. Si no tienes `OPENAI_API_KEY`, el notebook funcionará en modo `mock`.\n",
    "\n",
    "- Requisitos mínimos en `requirements.txt`.\n",
    "- Variables de entorno: `OPENAI_API_KEY` (opcional para modo mock).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e8b2e",
   "metadata": {},
   "source": [
    "```powershell\n",
    "# PowerShell: crear virtualenv e instalar dependencias\n",
    "python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "```python\n",
    "# Comprobar entorno y variables (celda ejecutable)\n",
    "import sys, os\n",
    "import importlib\n",
    "\n",
    "print('Python', sys.version.split()[0])\n",
    "print('OPENAI_API_KEY present:', 'OPENAI_API_KEY' in os.environ)\n",
    "\n",
    "# Mostrar versiones de paquetes clave si están instalados\n",
    "for pkg in ('numpy','pandas','openai'):\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        print(pkg, 'version', getattr(mod, '__version__', 'unknown'))\n",
    "    except Exception:\n",
    "        print(pkg, 'no instalado')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c58b59",
   "metadata": {},
   "source": [
    "## 2) Estructura y limpieza de redundancias\n",
    "\n",
    "En esta sección consolidamos la configuración y eliminamos duplicados del examen original:\n",
    "- Unificar la inicialización del cliente OpenAI en un solo bloque.\n",
    "- Definir constantes (modelos, paths) en una sola celda.\n",
    "- Reemplazar llamadas ad-hoc por funciones reutilizables (helpers).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec6a10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración unificada y utilidades\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuración básica\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths (ajustar según el repo)\n",
    "DATA_DIR = '.'\n",
    "KNOWLEDGE_CSV = os.path.join(DATA_DIR, 'knowledge_base.csv')\n",
    "PROCESSED_QUERIES = os.path.join(DATA_DIR, 'processed_queries.csv')\n",
    "PREDEFINED_RESPONSES = os.path.join(DATA_DIR, 'predefined_responses.json')\n",
    "CHATBOT_RESPONSES = os.path.join(DATA_DIR, 'chatbot_responses.json')\n",
    "\n",
    "# Modelos y parámetros\n",
    "EMBEDDING_MODEL = 'text-embedding-3-small'\n",
    "GPT_MODEL = 'gpt-3.5-turbo'\n",
    "SIMILARITY_THRESHOLD = 0.1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Modo: si no hay clave de OpenAI se usa MOCK\n",
    "USE_MOCK = 'OPENAI_API_KEY' not in os.environ\n",
    "\n",
    "# Helpers: retry wrapper\n",
    "def retry(func, retries=3, delay=2, *args, **kwargs):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt+1}/{retries} failed: {e}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Cosine similarity helper\n",
    "def cosine_similarity_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return cosine similarities between each row in a and vector b (or rows in b).\"\"\"\n",
    "    if a.ndim == 1:\n",
    "        a = a.reshape(1, -1)\n",
    "    if b.ndim == 1:\n",
    "        b = b.reshape(1, -1)\n",
    "    a_norm = np.linalg.norm(a, axis=1, keepdims=True)\n",
    "    b_norm = np.linalg.norm(b, axis=1, keepdims=True)\n",
    "    return (a @ b.T) / (a_norm * b_norm.T + 1e-10)\n",
    "\n",
    "# Mock embedding generator (deterministic) for testing sin API\n",
    "import hashlib\n",
    "\n",
    "def _mock_embedding(text: str, dim: int = 1536) -> np.ndarray:\n",
    "    # Deterministic pseudo-embedding using hash\n",
    "    h = hashlib.sha256(text.encode('utf-8')).digest()\n",
    "    vec = np.frombuffer(h, dtype=np.uint8).astype(np.float32)\n",
    "    vec = np.pad(vec, (0, max(0, dim - vec.size)), mode='wrap')[:dim]\n",
    "    return vec / (np.linalg.norm(vec) + 1e-10)\n",
    "\n",
    "# Wrapper for embeddings (mock or real)\n",
    "def make_embeddings(texts: List[str], model: str = EMBEDDING_MODEL, batch_size: int = BATCH_SIZE) -> List[np.ndarray]:\n",
    "    \"\"\"Genera embeddings: en modo mock usa _mock_embedding, si hay clave llama a la API real.\n",
    "    Devuelve una lista de numpy arrays.\"\"\"\n",
    "    if USE_MOCK:\n",
    "        return [_mock_embedding(t) for t in texts]\n",
    "    else:\n",
    "        # Aquí se colocaría la llamada a la API real (ej: client.embeddings.create)\n",
    "        from openai import OpenAI\n",
    "        client = OpenAI()\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            resp = client.embeddings.create(input=batch, model=model)\n",
    "            embeddings.extend([np.array(item.embedding) for item in resp.data])\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be74f36",
   "metadata": {},
   "source": [
    "## Stage 1 — Ingesta de datos y creación de embeddings\n",
    "\n",
    "En este stage mostramos cómo cargar `knowledge_base.csv`, preprocesar y crear embeddings de manera eficiente con batching y caching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para Stage 1: carga y preprocesado\n",
    "\n",
    "def load_documents_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    expected_cols = {'document_id', 'document_text'}\n",
    "    if not expected_cols.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"CSV debe contener las columnas: {expected_cols}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    if len(tokens) <= chunk_size:\n",
    "        return [text]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + chunk_size, len(tokens))\n",
    "        chunk = ' '.join(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        if end == len(tokens):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# Ejemplo: crear embeddings y guardarlos (modo mock si no hay API key)\n",
    "\n",
    "def stage1_create_embeddings(input_csv: str = KNOWLEDGE_CSV, output_json: str = 'knowledge_embeddings.json') -> None:\n",
    "    df = load_documents_csv(input_csv)\n",
    "    texts = df['document_text'].tolist()\n",
    "    embeddings = make_embeddings(texts)\n",
    "    df['embedding_vector'] = [vec.tolist() for vec in embeddings]\n",
    "    # Guardar en JSON simplificado\n",
    "    records = df.to_dict(orient='records')\n",
    "    with open(output_json, 'w', encoding='utf-8') as f:\n",
    "        json.dump(records, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f'Stage1: guardado {len(records)} embeddings en {output_json}')\n",
    "\n",
    "# Test rápido (si existe el archivo)\n",
    "try:\n",
    "    if os.path.exists(KNOWLEDGE_CSV):\n",
    "        print('Se detectó knowledge_base.csv; puedes ejecutar stage1_create_embeddings()')\n",
    "    else:\n",
    "        print('No se detectó knowledge_base.csv; el notebook puede usarse en modo mock.')\n",
    "except Exception as e:\n",
    "    print('Error comprobando archivos:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9cbc2",
   "metadata": {},
   "source": [
    "## Stage 2 — Construcción del índice y recuperación (RAG)\n",
    "\n",
    "Esta sección muestra un vector store simple usando numpy y persistencia a JSON (ligero, para demo). Para producción, reemplazar por FAISS/Annoy/Pinecone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a309803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: build vector store simple (numpy)\n",
    "\n",
    "def build_vector_store_from_embeddings(records: List[Dict[str, Any]], persist_path: str = 'vector_store.json') -> Dict[str, Any]:\n",
    "    \"\"\"Espera records con keys: document_id, document_text, embedding_vector\"\"\"\n",
    "    vectors = np.array([np.array(r['embedding_vector']) for r in records])\n",
    "    texts = [r.get('document_text') for r in records]\n",
    "    meta = [{'document_id': r['document_id']} for r in records]\n",
    "    store = {\n",
    "        'vectors': vectors.tolist(),\n",
    "        'texts': texts,\n",
    "        'meta': meta\n",
    "    }\n",
    "    with open(persist_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(store, f, indent=2, ensure_ascii=False)\n",
    "    logger.info(f'Persistido vector store en {persist_path} con {len(texts)} entradas')\n",
    "    return store\n",
    "\n",
    "\n",
    "def load_vector_store(persist_path: str = 'vector_store.json') -> Dict[str, Any]:\n",
    "    with open(persist_path, 'r', encoding='utf-8') as f:\n",
    "        store = json.load(f)\n",
    "    store['vectors'] = np.array(store['vectors'])\n",
    "    return store\n",
    "\n",
    "\n",
    "def retrieve(store: Dict[str, Any], query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "    q_emb = make_embeddings([query])[0]\n",
    "    sims = cosine_similarity_matrix(store['vectors'], q_emb).ravel()\n",
    "    idx = np.argsort(sims)[::-1][:top_k]\n",
    "    results = []\n",
    "    for i in idx:\n",
    "        results.append({'text': store['texts'][i], 'meta': store['meta'][i], 'score': float(sims[i])})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845584d6",
   "metadata": {},
   "source": [
    "## Stage 3 — Chatbot (RAG + fallback a generación)\n",
    "\n",
    "En este stage ensamblamos el pipeline: embedding de la query, recuperación (RAG), construir prompt con contexto y generar respuesta con LLM si hace falta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd495582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline end-to-end y funciones de generación\n",
    "\n",
    "def format_context(docs: List[Dict[str, Any]]) -> str:\n",
    "    return \"\\n\\n\".join([f\"- {d['text']} (score={d['score']:.3f})\" for d in docs])\n",
    "\n",
    "\n",
    "def build_prompt(context: str, user_query: str, system_template: str = None) -> str:\n",
    "    if system_template is None:\n",
    "        system_template = (\n",
    "            \"Eres un asistente de soporte al cliente. Usa el contexto para responder con precisión y brevedad.\\n\\nContexto:\\n{context}\\n\\nPregunta:\\n{query}\\n\\nRespuesta:\" \n",
    "        )\n",
    "    return system_template.format(context=context, query=user_query)\n",
    "\n",
    "\n",
    "def mock_generate(prompt: str) -> str:\n",
    "    # Respuesta mock para demo\n",
    "    return \"[MOCK GENERATED RESPONSE] Basado en el contexto: \" + prompt[:200]\n",
    "\n",
    "\n",
    "def pipeline_end_to_end(store: Dict[str, Any], query: str, top_k: int = 3):\n",
    "    docs = retrieve(store, query, top_k=top_k)\n",
    "    context = format_context(docs)\n",
    "    if docs and docs[0]['score'] >= SIMILARITY_THRESHOLD:\n",
    "        # RAG: usar la respuesta recuperada\n",
    "        return {'source': 'RAG', 'answer': docs[0]['text'], 'score': docs[0]['score']}\n",
    "    else:\n",
    "        # Fallback a generación (mock o real)\n",
    "        prompt = build_prompt(context, query)\n",
    "        if USE_MOCK:\n",
    "            answer = mock_generate(prompt)\n",
    "        else:\n",
    "            from openai import OpenAI\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(model=GPT_MODEL, messages=[{'role':'system','content':'You are a helpful assistant.'},{'role':'user','content':prompt}])\n",
    "            answer = resp.choices[0].message.content\n",
    "        return {'source': 'GEN', 'answer': answer, 'score': 0.0}\n",
    "\n",
    "# Demo mini end-to-end con store simulado si no hay persistido\n",
    "if os.path.exists('vector_store.json'):\n",
    "    store = load_vector_store('vector_store.json')\n",
    "else:\n",
    "    # crear un store de ejemplo con textos sencillos\n",
    "    sample_records = [\n",
    "        {'document_id': 1, 'document_text': 'Nuestro horario de soporte es 9-17 L-V.', 'embedding_vector': make_embeddings(['Nuestro horario de soporte es 9-17 L-V.'])[0].tolist()},\n",
    "        {'document_id': 2, 'document_text': 'Para devoluciones, contacte a soporte en 30 días.', 'embedding_vector': make_embeddings(['Para devoluciones, contacte a soporte en 30 días.'])[0].tolist()}\n",
    "    ]\n",
    "    build_vector_store_from_embeddings(sample_records, persist_path='vector_store.json')\n",
    "    store = load_vector_store('vector_store.json')\n",
    "\n",
    "# Ejemplos\n",
    "print(pipeline_end_to_end(store, '¿Cuáles son sus horas de soporte?'))\n",
    "print(pipeline_end_to_end(store, '¿Cómo solicito una devolución por producto defectuoso?'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb74e2",
   "metadata": {},
   "source": [
    "## Edge cases y consideraciones\n",
    "\n",
    "- Rate limits: usar batching y reintentos.\n",
    "- Normalización: comprobar normas de longitud y normalizar vectores.\n",
    "- Archivos faltantes: validar paths antes de ejecutar.\n",
    "- Diferencias de dimensión en embeddings: asegurar dim consistente.\n",
    "- Caching y persistencia del índice para ahorrar costes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f81ae97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pruebas rápidas (unitarias) — demo con asserts\n",
    "\n",
    "# Test: cosine_similarity_matrix\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]])\n",
    "b = np.array([1.0, 0.0, 0.0])\n",
    "res = cosine_similarity_matrix(a, b).ravel()\n",
    "assert abs(res[0] - 1.0) < 1e-6\n",
    "assert abs(res[1] - 0.0) < 1e-6\n",
    "print('Test de similitud coseno OK')\n",
    "\n",
    "# Test: chunk_text\n",
    "s = ' '.join([f'pal{i}' for i in range(1200)])\n",
    "chunks = chunk_text(s, chunk_size=200, overlap=20)\n",
    "assert isinstance(chunks, list) and len(chunks) > 1\n",
    "print('Test chunk_text OK')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
