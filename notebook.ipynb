{"cells":[{"source":"# Practical Exam: Automating Customer Support with OpenAI API\n\nYou work as an AI Engineer at ChatSolveAI, a company that provides automated customer support solutions. The company wants to improve response times and accuracy in answering customer queries by leveraging OpenAI’s GPT models.\n\nYour task is to build a chatbot that classifies customer queries, retrieves relevant responses, and logs interactions in a structured way. The chatbot will use text embeddings, similarity search, API calls, and conversation management techniques.\n\n\n**Please note:** \n\n1. The OpenAI Embeddings API supports passing a list of strings to the input parameter in a single request. This allows you to generate multiple embeddings at once without looping over individual elements, which can significantly improve efficiency and reduce the risk of hitting rate limits.\n\n2. When submitting your solution, you may see an error message reading 'Something went wrong while submitting your solution. Please try again.' This is because using the OpenAI API means code may take longer to run than code in our other Certifications. Please ignore this message while your code is still running.","metadata":{},"id":"adb2307c-b472-49f9-a630-7ff34483e7bc","cell_type":"markdown"},{"source":"# Run this cell before running your solution\n\n# Import necessary modules\nimport os\nfrom openai import OpenAI\nimport pandas as pd\nimport json\nimport time\nimport datetime\nimport numpy as np\n\n# Define the model to use\nmodel = \"gpt-3.5-turbo\"\n\n# Define the client\nclient = OpenAI()","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1761153775541,"lastExecutedByKernel":"30a90a9e-0085-4499-8e91-e90754d81c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Run this cell before running your solution\n\n# Import necessary modules\nimport os\nfrom openai import OpenAI\nimport pandas as pd\nimport json\nimport time\nimport datetime\nimport numpy as np\n\n# Define the model to use\nmodel = \"gpt-3.5-turbo\"\n\n# Define the client\nclient = OpenAI()"},"id":"e8e08036-f207-45b9-94c2-a79b3795dcd6","cell_type":"code","execution_count":43,"outputs":[]},{"source":"# Task 1\n\nChatSolveAI has provided a knowledge base (`knowledge_base.csv`) containing information about various products, services, and customer policies. To enhance search and query capabilities, you need to convert this data into embeddings and store them for efficient retrieval.\n\n- Load the dataset (`knowledge_base.csv`).\n- Generate text embeddings using OpenAI’s embedding model (`text-embedding-3-small`). Each document's `document_text` should be transformed into an embedding vector. \n- Store the generated embeddings in a structured format (`knowledge_embeddings.json`) with the following format available below.\n- Store the embedded data and associated metadata for retrieval.  \n\n### Format to store generated embeddings:\n```json\n[\n    {\n       \"document_id\": 1,\n       \"document_text\": \"Example document text.\",\n       \"embedding_vector\": [0.123, 0.456, ...],\n       \"metadata\": \"Additional document info\"\n    }\n]\n```\n\n### Data description: \n\n| Column Name       | Criteria                                                |\n|-------------------|---------------------------------------------------------|\n| document_id       | Integer. Unique identifier for each document. No missing values. |\n| document_text     | String. Text content of the knowledge base. Preprocessed and embedded. |\n| embedding_vector  | List. Embedding representation of the `document_text`. |\n| metadata          | String. Metadata for additional information. |\n","metadata":{},"id":"911cf27b","cell_type":"markdown"},{"source":"# 1. Cargar el dataset\ntry:\n    df = pd.read_csv('knowledge_base.csv')\nexcept FileNotFoundError:\n    print(\"Error: knowledge_base.csv no encontrado.\")\n    raise\n\n# Extraer la lista de textos para la incrustación por lotes (eficiencia)\ntexts_to_embed = df['document_text'].tolist()\nembedding_model = \"text-embedding-3-small\"\n\nprint(f\"Generando {len(texts_to_embed)} embeddings con {embedding_model} en una sola llamada API...\")\n\n# 2. Generar embeddings de texto usando la API de OpenAI\ntry:\n    response = client.embeddings.create(\n        input=texts_to_embed,\n        model=embedding_model\n    )\n    \n    # Extraer los vectores de embedding (el orden coincide con la lista de entrada)\n    embedding_vectors = [item.embedding for item in response.data]\n\nexcept Exception as e:\n    print(f\"Error al generar embeddings: {e}\")\n    # Si el cliente falla, la ejecución se detendrá aquí.\n    raise\n\n# Agregar los vectores de embedding al DataFrame\ndf['embedding_vector'] = embedding_vectors\n\n# 3. y 4. Estructurar y almacenar los embeddings generados\n# Crear la lista de diccionarios en el formato requerido\nknowledge_embeddings = []\nfor index, row in df.iterrows():\n    knowledge_embeddings.append({\n        \"document_id\": int(row['document_id']), # Asegurar tipo entero\n        \"document_text\": row['document_text'],\n        \"embedding_vector\": row['embedding_vector'],\n        \"metadata\": row['metadata']\n    })\n\n# Guardar los datos estructurados en knowledge_embeddings.json\noutput_file = 'knowledge_embeddings.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(knowledge_embeddings, f, indent=4)\n\nprint(f\"\\nÉxito: Se ha guardado el archivo {output_file} con {len(knowledge_embeddings)} documentos.\")\n# Imprimir el primer elemento para confirmar el formato, aunque el evaluador revisará el archivo.\nprint(\"\\nEstructura del primer elemento para verificación:\")\n#print(json.dumps(knowledge_embeddings[0], indent=4))","metadata":{"executionCancelledAt":null,"executionTime":2438,"lastExecutedAt":1761153777979,"lastExecutedByKernel":"30a90a9e-0085-4499-8e91-e90754d81c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# 1. Cargar el dataset\ntry:\n    df = pd.read_csv('knowledge_base.csv')\nexcept FileNotFoundError:\n    print(\"Error: knowledge_base.csv no encontrado.\")\n    raise\n\n# Extraer la lista de textos para la incrustación por lotes (eficiencia)\ntexts_to_embed = df['document_text'].tolist()\nembedding_model = \"text-embedding-3-small\"\n\nprint(f\"Generando {len(texts_to_embed)} embeddings con {embedding_model} en una sola llamada API...\")\n\n# 2. Generar embeddings de texto usando la API de OpenAI\ntry:\n    response = client.embeddings.create(\n        input=texts_to_embed,\n        model=embedding_model\n    )\n    \n    # Extraer los vectores de embedding (el orden coincide con la lista de entrada)\n    embedding_vectors = [item.embedding for item in response.data]\n\nexcept Exception as e:\n    print(f\"Error al generar embeddings: {e}\")\n    # Si el cliente falla, la ejecución se detendrá aquí.\n    raise\n\n# Agregar los vectores de embedding al DataFrame\ndf['embedding_vector'] = embedding_vectors\n\n# 3. y 4. Estructurar y almacenar los embeddings generados\n# Crear la lista de diccionarios en el formato requerido\nknowledge_embeddings = []\nfor index, row in df.iterrows():\n    knowledge_embeddings.append({\n        \"document_id\": int(row['document_id']), # Asegurar tipo entero\n        \"document_text\": row['document_text'],\n        \"embedding_vector\": row['embedding_vector'],\n        \"metadata\": row['metadata']\n    })\n\n# Guardar los datos estructurados en knowledge_embeddings.json\noutput_file = 'knowledge_embeddings.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(knowledge_embeddings, f, indent=4)\n\nprint(f\"\\nÉxito: Se ha guardado el archivo {output_file} con {len(knowledge_embeddings)} documentos.\")\n# Imprimir el primer elemento para confirmar el formato, aunque el evaluador revisará el archivo.\nprint(\"\\nEstructura del primer elemento para verificación:\")\n#print(json.dumps(knowledge_embeddings[0], indent=4))","outputsMetadata":{"0":{"height":122,"type":"stream"}}},"id":"e622234a-8f5f-42f3-9c02-39605971cdbb","cell_type":"code","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":"Generando 501 embeddings con text-embedding-3-small en una sola llamada API...\n\nÉxito: Se ha guardado el archivo knowledge_embeddings.json con 501 documentos.\n\nEstructura del primer elemento para verificación:\n"}]},{"source":"# Task 2\n\nChatSolveAI receives customer queries that need to be classified and matched with appropriate responses. Your task is to preprocess and embed these queries, perform similarity searches on predefined responses (contained in `predefined_responses.json`), and retrieve the most relevant responses.\n\n- Load the dataset (`processed_queries.csv`).\n- Retrieve responses by using cosine similarity to perform a similarity search against predefined responses in `predefined_responses.json`.\n- Structure API requests properly and implement error handling, including retry mechanisms to handle rate limits.\n- Format model responses as JSON to maintain consistency in output.\n- Compute confidence scores for retrieved responses, scaled to 0-1.\n- Store the structured responses in a JSON file (`query_responses.json`), suitable for integration with other applications. Your JSON file should be structured as follows:\n\n| Column Name       | Criteria                                                   |\n|-------------------|------------------------------------------------------------|\n| query_id         | Integer. Unique identifier for each query. No missing values. |\n| query_text       | String. Preprocessed query text. |\n| top_responses    | List. Top 3 most relevant responses retrieved. |\n| confidence_scores | List. Model-based confidence score for the top 3 responses. |","metadata":{},"id":"cc363bc4","cell_type":"markdown"},{"source":"from openai import APIError, RateLimitError # Importaciones de errores correctas\n\n# --- Inicialización y Constantes ---\ntry:\n    client = OpenAI()\nexcept Exception:\n    pass\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nMAX_RETRIES = 5\nRETRY_DELAY = 5 # Segundos\nTOP_K = 3 # Recuperar las 3 respuestas más relevantes\n\n# Función para realizar el llamado a la API con manejo de reintentos (esencial para el examen)\ndef get_embeddings_with_retry(texts, model=EMBEDDING_MODEL, max_retries=MAX_RETRIES):\n    \"\"\"\n    Genera embeddings para una lista de textos con manejo de reintentos.\n    \"\"\"\n    if not texts:\n        return []\n    \n    for attempt in range(max_retries):\n        try:\n            # Uso de input como lista de strings para batching (eficiencia)\n            response = client.embeddings.create(input=texts, model=model)\n            return [item.embedding for item in response.data]\n        except (RateLimitError, APIError) as e:\n            if attempt < max_retries - 1:\n                print(f\"Error ({type(e).__name__}). Retrying in {RETRY_DELAY}s...\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"Failed after {max_retries} attempts. Final error: {e}\")\n                raise\n\n# --- Tarea 2: Proceso Principal ---\n\ntry:\n    # 1. Cargar el dataset de queries y la base de respuestas\n    queries_df = pd.read_csv('processed_queries.csv')\n    query_texts = queries_df['query_text'].tolist()\n    \n    # Cargar la base de respuestas, asumiendo que el archivo correcto es predefined_responses.json\n    with open('predefined_responses.json', 'r', encoding='utf-8') as f:\n        response_data_raw = json.load(f)\n\n    # Lógica de manejo de estructura (basada en tu extracto de código)\n    response_data = []\n    if isinstance(response_data_raw, dict):\n        # Si es un diccionario, tomamos los valores (asumiendo que son los textos de respuesta)\n        response_texts = list(response_data_raw.values())\n        # Creamos una estructura consistente para trabajar: lista de diccionarios\n        response_data = [{'response_text': text} for text in response_texts]\n    elif isinstance(response_data_raw, list):\n        response_data = response_data_raw\n        if response_data and isinstance(response_data[0], str):\n            # Si es una lista de strings\n            response_texts = response_data\n            response_data = [{'response_text': text} for text in response_texts]\n        elif response_data and isinstance(response_data[0], dict) and 'response_text' in response_data[0]:\n            # Si es una lista de diccionarios con la clave 'response_text'\n            response_texts = [item['response_text'] for item in response_data]\n        else:\n            raise ValueError(\"Unsupported structure in predefined_responses.json.\")\n    else:\n        raise TypeError(\"predefined_responses.json must contain a JSON array or object.\")\n\n    \n    print(f\"Loaded {len(response_texts)} predefined responses.\")\n\n    # 2. Generar Embeddings\n    print(f\"Generating embeddings for {len(query_texts)} queries...\")\n    query_embeddings = get_embeddings_with_retry(query_texts)\n    \n    # **PUNTO DE CORRECCIÓN CLAVE**: Generar embeddings para la base de respuestas\n    print(f\"Generating embeddings for {len(response_texts)} responses...\")\n    response_embeddings = get_embeddings_with_retry(response_texts)\n    \n    # Añadir los embeddings a la estructura de la base de respuestas\n    for i, embedding in enumerate(response_embeddings):\n        # Aseguramos que cada elemento tenga el vector de embedding\n        response_data[i]['embedding_vector'] = embedding\n        \n    kb_vectors = np.array([item['embedding_vector'] for item in response_data])\n    kb_texts = [item['response_text'] for item in response_data] # La lista final de textos para la salida\n    \n    # 3. Buscar y Puntuar (Similitud del Coseno)\n    \n    query_vectors = np.array(query_embeddings)\n    final_results = []\n    \n    for i, (idx, row) in enumerate(queries_df.iterrows()):\n        query_text = row['query_text']\n        query_id = int(row['query_id'])\n        query_vector_np = query_vectors[i] \n\n        # Calcular la similitud del coseno\n        # Similitud = (A . B) / (||A|| * ||B||)\n        similarities = np.dot(kb_vectors, query_vector_np) / (\n            np.linalg.norm(kb_vectors, axis=1) * np.linalg.norm(query_vector_np)\n        )\n\n        # Obtener los índices de las TOP_K respuestas más relevantes\n        top_indices = np.argsort(similarities)[::-1][:TOP_K]\n\n        # Obtener respuestas y puntuaciones de confianza\n        confidence_scores = similarities[top_indices].tolist()\n        top_responses_list = [kb_texts[j] for j in top_indices]\n\n        # 4. Almacenar el resultado estructurado\n        final_results.append({\n            \"query_id\": query_id,\n            \"query_text\": query_text,\n            \"top_responses\": top_responses_list,\n            # Las puntuaciones de confianza son la similitud del coseno, escaladas 0-1\n            \"confidence_scores\": [float(score) for score in confidence_scores] \n        })\n\n    # 5. Formatear y Guardar\n    output_file = 'query_responses.json'\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(final_results, f, indent=4)\n        \n    print(f\"\\nSuccessfully processed and matched {len(final_results)} queries.\")\n    print(f\"Results saved to '{output_file}'.\")\n    # Imprimir el primer elemento para verificación\n    print(json.dumps(final_results[0], indent=4))\n\nexcept Exception as e:\n    # Captura cualquier error no manejado\n    print(f\"An unexpected error occurred during Task 2. Review environment setup and file paths. Error: {str(e)}\")\n    raise","metadata":{"executionCancelledAt":null,"executionTime":2137,"lastExecutedAt":1761153780116,"lastExecutedByKernel":"30a90a9e-0085-4499-8e91-e90754d81c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from openai import APIError, RateLimitError # Importaciones de errores correctas\n\n# --- Inicialización y Constantes ---\ntry:\n    client = OpenAI()\nexcept Exception:\n    pass\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nMAX_RETRIES = 5\nRETRY_DELAY = 5 # Segundos\nTOP_K = 3 # Recuperar las 3 respuestas más relevantes\n\n# Función para realizar el llamado a la API con manejo de reintentos (esencial para el examen)\ndef get_embeddings_with_retry(texts, model=EMBEDDING_MODEL, max_retries=MAX_RETRIES):\n    \"\"\"\n    Genera embeddings para una lista de textos con manejo de reintentos.\n    \"\"\"\n    if not texts:\n        return []\n    \n    for attempt in range(max_retries):\n        try:\n            # Uso de input como lista de strings para batching (eficiencia)\n            response = client.embeddings.create(input=texts, model=model)\n            return [item.embedding for item in response.data]\n        except (RateLimitError, APIError) as e:\n            if attempt < max_retries - 1:\n                print(f\"Error ({type(e).__name__}). Retrying in {RETRY_DELAY}s...\")\n                time.sleep(RETRY_DELAY)\n            else:\n                print(f\"Failed after {max_retries} attempts. Final error: {e}\")\n                raise\n\n# --- Tarea 2: Proceso Principal ---\n\ntry:\n    # 1. Cargar el dataset de queries y la base de respuestas\n    queries_df = pd.read_csv('processed_queries.csv')\n    query_texts = queries_df['query_text'].tolist()\n    \n    # Cargar la base de respuestas, asumiendo que el archivo correcto es predefined_responses.json\n    with open('predefined_responses.json', 'r', encoding='utf-8') as f:\n        response_data_raw = json.load(f)\n\n    # Lógica de manejo de estructura (basada en tu extracto de código)\n    response_data = []\n    if isinstance(response_data_raw, dict):\n        # Si es un diccionario, tomamos los valores (asumiendo que son los textos de respuesta)\n        response_texts = list(response_data_raw.values())\n        # Creamos una estructura consistente para trabajar: lista de diccionarios\n        response_data = [{'response_text': text} for text in response_texts]\n    elif isinstance(response_data_raw, list):\n        response_data = response_data_raw\n        if response_data and isinstance(response_data[0], str):\n            # Si es una lista de strings\n            response_texts = response_data\n            response_data = [{'response_text': text} for text in response_texts]\n        elif response_data and isinstance(response_data[0], dict) and 'response_text' in response_data[0]:\n            # Si es una lista de diccionarios con la clave 'response_text'\n            response_texts = [item['response_text'] for item in response_data]\n        else:\n            raise ValueError(\"Unsupported structure in predefined_responses.json.\")\n    else:\n        raise TypeError(\"predefined_responses.json must contain a JSON array or object.\")\n\n    \n    print(f\"Loaded {len(response_texts)} predefined responses.\")\n\n    # 2. Generar Embeddings\n    print(f\"Generating embeddings for {len(query_texts)} queries...\")\n    query_embeddings = get_embeddings_with_retry(query_texts)\n    \n    # **PUNTO DE CORRECCIÓN CLAVE**: Generar embeddings para la base de respuestas\n    print(f\"Generating embeddings for {len(response_texts)} responses...\")\n    response_embeddings = get_embeddings_with_retry(response_texts)\n    \n    # Añadir los embeddings a la estructura de la base de respuestas\n    for i, embedding in enumerate(response_embeddings):\n        # Aseguramos que cada elemento tenga el vector de embedding\n        response_data[i]['embedding_vector'] = embedding\n        \n    kb_vectors = np.array([item['embedding_vector'] for item in response_data])\n    kb_texts = [item['response_text'] for item in response_data] # La lista final de textos para la salida\n    \n    # 3. Buscar y Puntuar (Similitud del Coseno)\n    \n    query_vectors = np.array(query_embeddings)\n    final_results = []\n    \n    for i, (idx, row) in enumerate(queries_df.iterrows()):\n        query_text = row['query_text']\n        query_id = int(row['query_id'])\n        query_vector_np = query_vectors[i] \n\n        # Calcular la similitud del coseno\n        # Similitud = (A . B) / (||A|| * ||B||)\n        similarities = np.dot(kb_vectors, query_vector_np) / (\n            np.linalg.norm(kb_vectors, axis=1) * np.linalg.norm(query_vector_np)\n        )\n\n        # Obtener los índices de las TOP_K respuestas más relevantes\n        top_indices = np.argsort(similarities)[::-1][:TOP_K]\n\n        # Obtener respuestas y puntuaciones de confianza\n        confidence_scores = similarities[top_indices].tolist()\n        top_responses_list = [kb_texts[j] for j in top_indices]\n\n        # 4. Almacenar el resultado estructurado\n        final_results.append({\n            \"query_id\": query_id,\n            \"query_text\": query_text,\n            \"top_responses\": top_responses_list,\n            # Las puntuaciones de confianza son la similitud del coseno, escaladas 0-1\n            \"confidence_scores\": [float(score) for score in confidence_scores] \n        })\n\n    # 5. Formatear y Guardar\n    output_file = 'query_responses.json'\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(final_results, f, indent=4)\n        \n    print(f\"\\nSuccessfully processed and matched {len(final_results)} queries.\")\n    print(f\"Results saved to '{output_file}'.\")\n    # Imprimir el primer elemento para verificación\n    print(json.dumps(final_results[0], indent=4))\n\nexcept Exception as e:\n    # Captura cualquier error no manejado\n    print(f\"An unexpected error occurred during Task 2. Review environment setup and file paths. Error: {str(e)}\")\n    raise","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"id":"b0a8525a-4279-4c1c-8d94-3780b6799355","cell_type":"code","execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":"Loaded 19 predefined responses.\nGenerating embeddings for 501 queries...\nGenerating embeddings for 19 responses...\n\nSuccessfully processed and matched 501 queries.\nResults saved to 'query_responses.json'.\n{\n    \"query_id\": 1,\n    \"query_text\": \"How can I contact customer support?\",\n    \"top_responses\": [\n        \"You can contact our customer support via email or live chat on our website.\",\n        \"If you received a damaged product, please contact support with images for a replacement.\",\n        \"Track your order by logging into your account and checking the 'Orders' section.\"\n    ],\n    \"confidence_scores\": [\n        0.6672946268827012,\n        0.37270208279765044,\n        0.330629818694895\n    ]\n}\n"}]},{"source":"# Task 3\n\nTo provide seamless customer service, ChatSolveAI wants to develop a chatbot that can respond to customer queries efficiently by searching for relevant responses and generating new ones when necessary.\n\n- Develop a chatbot that:\n    - Accepts customer queries via text input.\n    - Searches for the most relevant responses from a predefined set of responses (`chatbot_responses.json`).\n    - Uses the OpenAI Embeddings API (`text-embedding-3-small`) to compute semantic similarity between queries.\n    - If no relevant response is found from the predefined set, generates a new response using GPT-3.5-turbo.\n- Stores conversation history, including:\n    - Query text\n    - Retrieved response\n    - Timestamp of the interaction\n    - Confidence score of the response\n- Include one open-ended query not in the predefined responses (e.g., about the refund policy) to test the chatbot’s ability to handle unmatched queries.\n- Include one paraphrased query about support hours (e.g., “When can I talk to someone from support?”) to test semantic similarity matching.\n- Store structured chatbot responses in a JSON file (`sample_chatbot_responses.json`). Make sure they follow this format:\n```json\n[\n    {\n        \"query_text\": \"How do I reset my password?\",\n        \"retrieved_response\": \"You can reset your password by clicking 'Forgot Password' on the login page.\",\n        \"timestamp\": \"2025-04-02T14:30:00Z\",\n        \"confidence_score\": 0.92\n    },\n    {\n        \"query_text\": \"What are your business hours?\",\n        \"retrieved_response\": \"Our support team is available from 9 AM to 5 PM, Monday to Friday.\",\n        \"timestamp\": \"2025-04-02T14:35:00Z\",\n        \"confidence_score\": 0.87\n    }\n]\n```","metadata":{},"id":"01aca3b9","cell_type":"markdown"},{"source":"try:\n    client = OpenAI()\n    GPT_MODEL = \"gpt-3.5-turbo\"\nexcept Exception:\n    pass\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nSIMILARITY_THRESHOLD = 0.1  # Umbral de similitud para la recuperación RAG\n\n# Función simplificada para obtener embedding (sin reintentos, como solicitaste)\ndef get_embedding(text):\n    \"\"\"Genera el embedding para un único texto.\"\"\"\n    # Nota: Se usa un listado de un solo elemento para el parámetro 'input'\n    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n    return np.array(response.data[0].embedding)\n\ndef get_gpt_response(query_text, model=GPT_MODEL):\n    \"\"\"Genera una respuesta usando el modelo GPT-3.5-turbo.\"\"\"\n    # Prompt de sistema para el rol de soporte al cliente\n    system_prompt = \"You are a helpful and friendly customer support agent for ChatSolveAI. Provide concise and accurate answers.\"\n    \n    # Manejo de la llamada a la API con una pequeña pausa para evitar límites de tasa\n    time.sleep(1) # Pequeña pausa opcional para evitar límite de tasa en GPT-3.5-turbo\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": query_text}\n        ]\n    )\n    return response.choices[0].message.content.strip()\n\n# --- Base de Datos y Funciones RAG ---\n\n# 1. Cargar la base de respuestas predefinidas (chatbot_responses.json)\ntry:\n    with open('chatbot_responses.json', 'r', encoding='utf-8') as f:\n        chatbot_responses = json.load(f)\nexcept FileNotFoundError:\n    print(\"Error: chatbot_responses.json no encontrado. Necesario para el RAG.\")\n    raise\n\n# 2. Generar embeddings para las consultas predefinidas si no existen (pre-procesamiento de la KB)\nprint(f\"Generando embeddings para {len(chatbot_responses)} consultas predefinidas...\")\n\nkb_query_texts = [item['query_text'] for item in chatbot_responses]\n\n# Generar todos los embeddings en un solo lote para eficiencia\nbatch_response = client.embeddings.create(input=kb_query_texts, model=EMBEDDING_MODEL)\nkb_vectors = np.array([item.embedding for item in batch_response.data])\n\n# Extraer las respuestas asociadas (son las 'retrieved_response' del archivo)\nkb_response_texts = [item['retrieved_response'] for item in chatbot_responses]\n\n\ndef find_best_match(query_vector):\n    \"\"\"Busca la respuesta más relevante por similitud del coseno.\"\"\"\n    \n    # 3. Calcular similitudes\n    # Fórmula optimizada usando numpy para el producto punto normalizado\n    # Similitud del Coseno = (A . B) / (||A|| * ||B||)\n    similarities = np.dot(kb_vectors, query_vector) / (\n        np.linalg.norm(kb_vectors, axis=1) * np.linalg.norm(query_vector)\n    )\n\n    # Encontrar el índice de la respuesta más similar y su puntuación\n    max_index = np.argmax(similarities)\n    max_similarity = similarities[max_index]\n    \n    return kb_response_texts[max_index], float(max_similarity)\n\ndef process_query(query_text):\n    \"\"\"Procesa una consulta: RAG si es relevante, GPT si no lo es.\"\"\"\n    \n    # 1. Generar embedding para la consulta de entrada\n    query_vector = get_embedding(query_text)\n    \n    # 2. Buscar el mejor match en la KB\n    best_response, max_similarity = find_best_match(query_vector)\n    \n    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z')\n\n    if max_similarity >= SIMILARITY_THRESHOLD:\n        # 4. RAG: Respuesta recuperada de la base de datos\n        retrieved_response = best_response\n        confidence_score = max_similarity \n        \n    else:\n        # 5. Generación: No se encontró respuesta relevante, llamar a GPT\n        retrieved_response = get_gpt_response(query_text)\n        confidence_score = 0.0 # Confianza de 0.0 para respuestas generadas\n    \n    # 6. Estructurar la conversación\n    return {\n        \"query_text\": query_text,\n        \"retrieved_response\": retrieved_response,\n        \"timestamp\": timestamp,\n        \"confidence_score\": confidence_score\n    }\n\n# --- Ejecución y Registro ---\n\n# Consultas de prueba requeridas\ntest_queries = [\n    # 1. Consulta PARAFRASEADA (debe coincidir con RAG, alta confianza)\n    \"When can I talk to someone from support?\",\n    # 2. Consulta ABIERTA (debe fallar el umbral y usar GPT, confianza 0.0)\n    \"What is the policy for getting a refund on a defective product?\"\n]\n\nconversation_history = []\n\nfor query in test_queries:\n    response_data = process_query(query)\n    \n    # Almacenar el historial\n    conversation_history.append(response_data)\n    \n    # Impresión para verificación (opcional, pero útil en el notebook)\n    print(\"-\" * 50)\n    print(f\"QUERY: {response_data['query_text']}\")\n    print(f\"RESPONSE TYPE: {'RAG' if response_data['confidence_score'] > 0.0 else 'GPT GENERATION'}\")\n    print(f\"RESPONSE: {response_data['retrieved_response'][:70]}...\")\n    print(f\"CONFIDENCE: {response_data['confidence_score']:.4f}\")\n\n# 8. Guardar el historial en el archivo JSON\noutput_file = 'sample_chatbot_responses.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(conversation_history, f, indent=4)\n\nprint(\"-\" * 50)\nprint(f\"Éxito: Se ha guardado el historial de conversación en {output_file}.\")\nprint(\"Estructura de la conversación para la primera consulta:\")\nprint(json.dumps(conversation_history[1], indent=4))","metadata":{"executionCancelledAt":null,"executionTime":1264,"lastExecutedAt":1761153781380,"lastExecutedByKernel":"30a90a9e-0085-4499-8e91-e90754d81c28","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"try:\n    client = OpenAI()\n    GPT_MODEL = \"gpt-3.5-turbo\"\nexcept Exception:\n    pass\n\nEMBEDDING_MODEL = \"text-embedding-3-small\"\nSIMILARITY_THRESHOLD = 0.1  # Umbral de similitud para la recuperación RAG\n\n# Función simplificada para obtener embedding (sin reintentos, como solicitaste)\ndef get_embedding(text):\n    \"\"\"Genera el embedding para un único texto.\"\"\"\n    # Nota: Se usa un listado de un solo elemento para el parámetro 'input'\n    response = client.embeddings.create(input=[text], model=EMBEDDING_MODEL)\n    return np.array(response.data[0].embedding)\n\ndef get_gpt_response(query_text, model=GPT_MODEL):\n    \"\"\"Genera una respuesta usando el modelo GPT-3.5-turbo.\"\"\"\n    # Prompt de sistema para el rol de soporte al cliente\n    system_prompt = \"You are a helpful and friendly customer support agent for ChatSolveAI. Provide concise and accurate answers.\"\n    \n    # Manejo de la llamada a la API con una pequeña pausa para evitar límites de tasa\n    time.sleep(1) # Pequeña pausa opcional para evitar límite de tasa en GPT-3.5-turbo\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": query_text}\n        ]\n    )\n    return response.choices[0].message.content.strip()\n\n# --- Base de Datos y Funciones RAG ---\n\n# 1. Cargar la base de respuestas predefinidas (chatbot_responses.json)\ntry:\n    with open('chatbot_responses.json', 'r', encoding='utf-8') as f:\n        chatbot_responses = json.load(f)\nexcept FileNotFoundError:\n    print(\"Error: chatbot_responses.json no encontrado. Necesario para el RAG.\")\n    raise\n\n# 2. Generar embeddings para las consultas predefinidas si no existen (pre-procesamiento de la KB)\nprint(f\"Generando embeddings para {len(chatbot_responses)} consultas predefinidas...\")\n\nkb_query_texts = [item['query_text'] for item in chatbot_responses]\n\n# Generar todos los embeddings en un solo lote para eficiencia\nbatch_response = client.embeddings.create(input=kb_query_texts, model=EMBEDDING_MODEL)\nkb_vectors = np.array([item.embedding for item in batch_response.data])\n\n# Extraer las respuestas asociadas (son las 'retrieved_response' del archivo)\nkb_response_texts = [item['retrieved_response'] for item in chatbot_responses]\n\n\ndef find_best_match(query_vector):\n    \"\"\"Busca la respuesta más relevante por similitud del coseno.\"\"\"\n    \n    # 3. Calcular similitudes\n    # Fórmula optimizada usando numpy para el producto punto normalizado\n    # Similitud del Coseno = (A . B) / (||A|| * ||B||)\n    similarities = np.dot(kb_vectors, query_vector) / (\n        np.linalg.norm(kb_vectors, axis=1) * np.linalg.norm(query_vector)\n    )\n\n    # Encontrar el índice de la respuesta más similar y su puntuación\n    max_index = np.argmax(similarities)\n    max_similarity = similarities[max_index]\n    \n    return kb_response_texts[max_index], float(max_similarity)\n\ndef process_query(query_text):\n    \"\"\"Procesa una consulta: RAG si es relevante, GPT si no lo es.\"\"\"\n    \n    # 1. Generar embedding para la consulta de entrada\n    query_vector = get_embedding(query_text)\n    \n    # 2. Buscar el mejor match en la KB\n    best_response, max_similarity = find_best_match(query_vector)\n    \n    timestamp = datetime.datetime.now(datetime.timezone.utc).isoformat().replace('+00:00', 'Z')\n\n    if max_similarity >= SIMILARITY_THRESHOLD:\n        # 4. RAG: Respuesta recuperada de la base de datos\n        retrieved_response = best_response\n        confidence_score = max_similarity \n        \n    else:\n        # 5. Generación: No se encontró respuesta relevante, llamar a GPT\n        retrieved_response = get_gpt_response(query_text)\n        confidence_score = 0.0 # Confianza de 0.0 para respuestas generadas\n    \n    # 6. Estructurar la conversación\n    return {\n        \"query_text\": query_text,\n        \"retrieved_response\": retrieved_response,\n        \"timestamp\": timestamp,\n        \"confidence_score\": confidence_score\n    }\n\n# --- Ejecución y Registro ---\n\n# Consultas de prueba requeridas\ntest_queries = [\n    # 1. Consulta PARAFRASEADA (debe coincidir con RAG, alta confianza)\n    \"When can I talk to someone from support?\",\n    # 2. Consulta ABIERTA (debe fallar el umbral y usar GPT, confianza 0.0)\n    \"What is the policy for getting a refund on a defective product?\"\n]\n\nconversation_history = []\n\nfor query in test_queries:\n    response_data = process_query(query)\n    \n    # Almacenar el historial\n    conversation_history.append(response_data)\n    \n    # Impresión para verificación (opcional, pero útil en el notebook)\n    print(\"-\" * 50)\n    print(f\"QUERY: {response_data['query_text']}\")\n    print(f\"RESPONSE TYPE: {'RAG' if response_data['confidence_score'] > 0.0 else 'GPT GENERATION'}\")\n    print(f\"RESPONSE: {response_data['retrieved_response'][:70]}...\")\n    print(f\"CONFIDENCE: {response_data['confidence_score']:.4f}\")\n\n# 8. Guardar el historial en el archivo JSON\noutput_file = 'sample_chatbot_responses.json'\nwith open(output_file, 'w', encoding='utf-8') as f:\n    json.dump(conversation_history, f, indent=4)\n\nprint(\"-\" * 50)\nprint(f\"Éxito: Se ha guardado el historial de conversación en {output_file}.\")\nprint(\"Estructura de la conversación para la primera consulta:\")\nprint(json.dumps(conversation_history[1], indent=4))","outputsMetadata":{"0":{"height":458,"type":"stream"}}},"id":"3282c024-5414-4c3b-8e7b-06695175652d","cell_type":"code","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"Generando embeddings para 19 consultas predefinidas...\n--------------------------------------------------\nQUERY: When can I talk to someone from support?\nRESPONSE TYPE: RAG\nRESPONSE: Our support team is available from 9 AM to 5 PM, Monday to Friday....\nCONFIDENCE: 0.6821\n--------------------------------------------------\nQUERY: What is the policy for getting a refund on a defective product?\nRESPONSE TYPE: RAG\nRESPONSE: Yes, refunds are available within 30 days of purchase if you meet our ...\nCONFIDENCE: 0.4964\n--------------------------------------------------\nÉxito: Se ha guardado el historial de conversación en sample_chatbot_responses.json.\nEstructura de la conversación para la primera consulta:\n{\n    \"query_text\": \"What is the policy for getting a refund on a defective product?\",\n    \"retrieved_response\": \"Yes, refunds are available within 30 days of purchase if you meet our refund policy criteria.\",\n    \"timestamp\": \"2025-10-22T17:23:01.359952Z\",\n    \"confidence_score\": 0.49636265140708685\n}\n"}]}],"metadata":{"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}